{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaggregate correct/incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in Bool array yhat == ytrue\n",
    "with open(\"y_match_bool\", \"rb\") as f:\n",
    "    y_match_bool = pickle.load(f)\n",
    "    \n",
    "# read in Bool array for unreliable tweets\n",
    "with open(\"y_unrel\", \"rb\") as f:\n",
    "    is_unrel = pickle.load(f)\n",
    "\n",
    "# read in Bool array for reliable tweets\n",
    "with open(\"y_rel\", \"rb\") as f:\n",
    "    is_rel = pickle.load(f)\n",
    "    \n",
    "y_mismatch = (y_match_bool == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in explanations list\n",
    "with open(\"word_ica_netVals\", \"rb\") as f:\n",
    "    explanations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buy', 0.051218169268151266),\n",
       " ('today', 0.04564912805488858),\n",
       " ('recovered', 0.03684671343606235),\n",
       " ('coronavirus', 0.03167357242077659),\n",
       " ('2700', 0.030506518954913325),\n",
       " ('stocks', 0.029581742274224374),\n",
       " ('Reasons', 0.028743866428244537),\n",
       " ('deaths', 0.027860092852483344),\n",
       " ('virus', 0.025716989358801717),\n",
       " ('markets', 0.022864906695504512),\n",
       " ('1500', 0.02231262230616804),\n",
       " ('60', 0.021412463259315447),\n",
       " ('portfolio', 0.016436537908448373),\n",
       " ('back', 0.01619796505973802),\n",
       " ('new', 0.013312404914688995),\n",
       " ('cases', 0.01311502314673957),\n",
       " ('to', -0.01199561987952933)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by correctness of prediction\n",
    "from itertools import compress\n",
    "correctPred = list(compress(explanations, y_match_bool))\n",
    "wrongPred = list(compress(explanations, y_mismatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meth', 0.030987877447655538),\n",
       " ('your', 0.025280368484429324),\n",
       " ('for', 0.023607036528363858),\n",
       " ('This', 0.021277121458002424),\n",
       " ('with', 0.02115291971117502),\n",
       " ('coronavirus', 0.020399463946433768),\n",
       " ('contaminated', 0.020293806094854105),\n",
       " ('Florida', 0.01895198653657757),\n",
       " ('police', 0.01784072679125687),\n",
       " ('Is', 0.017506689596506714),\n",
       " ('will', 0.0156057483026649),\n",
       " ('dept', 0.011637419697194519),\n",
       " ('test', 0.011128987757850867),\n",
       " ('free', 0.005326991149549698),\n",
       " ('it', 0.002893041845161037)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongPred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique words from all explanations\n",
    "with open('tweets_vocab_list', 'rb') as f:\n",
    "    vocab_list = pickle.load(f)\n",
    "\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words from explanations for correctly predicted unreliable tweets\n",
    "vocab_correct = []\n",
    "for subList in correctPred:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_correct:\n",
    "            vocab_correct.append(el[0])\n",
    "len(vocab_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words from explanations for incorrectly predicted unreliable tweets\n",
    "vocab_wrong = []\n",
    "for subList in wrongPred:\n",
    "    for el in subList:\n",
    "        if el[0] not in vocab_wrong:\n",
    "            vocab_wrong.append(el[0])\n",
    "len(vocab_wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Table 1 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "t1 = [\n",
    "    'blame',\n",
    "    'accuse',\n",
    "    'refuse',\n",
    "    'catastrophe',\n",
    "    'chaos',\n",
    "    'evil',\n",
    "    'fight',\n",
    "    'danger',\n",
    "    'hysteria',\n",
    "    'panic',\n",
    "    'paranoia',\n",
    "    'laugh',\n",
    "    'stupidity',\n",
    "    'hear',\n",
    "    'see',\n",
    "    'feel',\n",
    "    'suppose',\n",
    "    'perceive',\n",
    "    'look',\n",
    "    'appear',\n",
    "    'suggest',\n",
    "    'believe',\n",
    "    'pretend',\n",
    "    'martial',\n",
    "    'kill',\n",
    "    'die',\n",
    "    'weapon',\n",
    "    'weaponizing',\n",
    "    'ussr',\n",
    "    'japan',\n",
    "    'fukushima',\n",
    "    'chernobyl',\n",
    "    'wuhan',\n",
    "    'china',\n",
    "    'foreigners',\n",
    "    'cats',\n",
    "    'dogs',\n",
    "    'i',\n",
    "    'me',\n",
    "    'mine',\n",
    "    'my',\n",
    "    'you',\n",
    "    'your',\n",
    "    'we',\n",
    "    'our',\n",
    "    'propaganda',\n",
    "    'fake',\n",
    "    'conspiracy',\n",
    "    'claim',\n",
    "    'misleading',\n",
    "    'hoax',\n",
    "    'cure',\n",
    "    'breakthrough',\n",
    "    'bitch',\n",
    "    'wtf',\n",
    "    'dogbreath',\n",
    "    'zombie',\n",
    "    'junkies',\n",
    "    'hell',\n",
    "    'screwed',\n",
    "    'secular',\n",
    "    'bible',\n",
    "    'maga',\n",
    "    'magat',\n",
    "    'genetic',\n",
    "    'hillary',\n",
    "    'chinese',\n",
    "    'fundamentalist',\n",
    "    'market',\n",
    "    'communist',\n",
    "    'nazi',\n",
    "    'stock',\n",
    "    'economy',\n",
    "    'money',\n",
    "    'cost',\n",
    "    'costs',\n",
    "    'election',\n",
    "    'campaign',\n",
    "    'presidential',\n",
    "    'impeachment',\n",
    "    'rallies',\n",
    "    'base',\n",
    "    'trump',\n",
    "    'war',\n",
    "    'iran'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320261437908496"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320261437908496"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296296296296297"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6296296296296297"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if el.lower() in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'we',\n",
       " 'Trump',\n",
       " 'You',\n",
       " 'your',\n",
       " 'look',\n",
       " 'TRUMP',\n",
       " 'We',\n",
       " 'My',\n",
       " 'fight',\n",
       " 'evil',\n",
       " 'Iran',\n",
       " 'chaos',\n",
       " 'refuse',\n",
       " 'me',\n",
       " 'Misleading',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'Market',\n",
       " 'market',\n",
       " 'stock',\n",
       " 'Wuhan',\n",
       " 'China',\n",
       " 'conspiracy',\n",
       " 'panic',\n",
       " 'Stupidity',\n",
       " 'Hell',\n",
       " 'Paranoia',\n",
       " 'weaponizing',\n",
       " 'hoax',\n",
       " 'campaign',\n",
       " 'my',\n",
       " 'I',\n",
       " 'our',\n",
       " 'rallies',\n",
       " 'base',\n",
       " 'screwed',\n",
       " 'weapon',\n",
       " 'Rallies',\n",
       " 'laugh',\n",
       " 'cure',\n",
       " 'DIE',\n",
       " 'Hoax',\n",
       " 'economy',\n",
       " 'Chinese',\n",
       " 'see',\n",
       " 'pretend',\n",
       " 'suggest',\n",
       " 'cost',\n",
       " 'Communist',\n",
       " 'kill',\n",
       " 'hysteria',\n",
       " 'Fake',\n",
       " 'war',\n",
       " 'impeachment',\n",
       " 'Conspiracy',\n",
       " 'Cure',\n",
       " 'FAKE',\n",
       " 'fake',\n",
       " 'ECONOMY',\n",
       " 'MARKET',\n",
       " 'STOCK',\n",
       " 'money',\n",
       " 'Chernobyl',\n",
       " 'USSR',\n",
       " 'Fukushima',\n",
       " 'Japan',\n",
       " 'KILL',\n",
       " 'MY',\n",
       " 'feel',\n",
       " 'believe',\n",
       " 'MAGA',\n",
       " 'Propaganda']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166666666666667"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7166666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2:  Table 1 + manual additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "t1PlusManual = [\n",
    "    'blame',\n",
    "    'accuse',\n",
    "    'refuse',\n",
    "    'catastrophe',\n",
    "    'emergency',\n",
    "    'chaos',\n",
    "    'crisis',\n",
    "    'evil',\n",
    "    'fight',\n",
    "    'danger',\n",
    "    'hysteria',\n",
    "    'panic',\n",
    "    'paranoia',\n",
    "    'fear',\n",
    "    'fears',\n",
    "    'laugh',\n",
    "    'stupidity',\n",
    "    'hear',\n",
    "    'see',\n",
    "    'feel',\n",
    "    'suppose',\n",
    "    'perceive',\n",
    "    'look',\n",
    "    'appear',\n",
    "    'suggest',\n",
    "    'believe',\n",
    "    'believed',\n",
    "    'pretend',\n",
    "    'martial',\n",
    "    'kill',\n",
    "    'killing',\n",
    "    'kills',\n",
    "    'killed',\n",
    "    'die',\n",
    "    'death',\n",
    "    'dies',\n",
    "    'dying',\n",
    "    'dead',\n",
    "    'died',\n",
    "    'threat',\n",
    "    'weapon',\n",
    "    'weaponize',\n",
    "    'weaponizing',\n",
    "    'knife',\n",
    "    'ussr',\n",
    "    'japan',\n",
    "    'chernobyl',\n",
    "    'wuhan',\n",
    "    'china',\n",
    "    'foreigners',\n",
    "    'cat',\n",
    "    'cats',\n",
    "    'dog',\n",
    "    'dogs',\n",
    "    'i',\n",
    "    'me',\n",
    "    'mine',\n",
    "    'my',\n",
    "    'you',\n",
    "    'yours',\n",
    "    'your',\n",
    "    'we',\n",
    "    'our',\n",
    "    'propaganda',\n",
    "    'fake',\n",
    "    'conspiracy',\n",
    "    'claim',\n",
    "    'claims',\n",
    "    'claiming',\n",
    "    'claimed',\n",
    "    'misleading',\n",
    "    'hoax',\n",
    "    'cure',\n",
    "    'breakthrough',\n",
    "    'bitch',\n",
    "    'wtf',\n",
    "    'dogbreath',\n",
    "    'zombie',\n",
    "    'junkies',\n",
    "    'hell',\n",
    "    'screwed',\n",
    "    'fuck',\n",
    "    'fucking',\n",
    "    'fucked',\n",
    "    'fuckin',\n",
    "    'wth',\n",
    "    'secular',\n",
    "    'bible',\n",
    "    'maga',\n",
    "    'magat',\n",
    "    'genetic',\n",
    "    'hillary',\n",
    "    'clinton',\n",
    "    'fundamentalist',\n",
    "    'market',\n",
    "    'communist',\n",
    "    'nazi',\n",
    "    'stock',\n",
    "    'bank',\n",
    "    'economy',\n",
    "    'economic',\n",
    "    'money',\n",
    "    'cost',\n",
    "    'costs',\n",
    "    'election',\n",
    "    'campaign',\n",
    "    'presidential',\n",
    "    'impeachment',\n",
    "    'rally',\n",
    "    'rallies',\n",
    "    'base',\n",
    "    'president',\n",
    "    'trump',\n",
    "    'war',\n",
    "    'wwiii',\n",
    "    'asteroid',\n",
    "    'banknotes',\n",
    "    'dangerous',\n",
    "    'invent',\n",
    "    'invented',\n",
    "    'iran',\n",
    "    'lie',\n",
    "    'lies',\n",
    "    'lying',\n",
    "    'lied',\n",
    "    'liar',\n",
    "    'liars',\n",
    "    'lmfao',\n",
    "    'lmfaoooooo',\n",
    "    'misinformation',\n",
    "    'news',\n",
    "    'media',\n",
    "    'financial',\n",
    "    'propagandawars',\n",
    "    'antidote'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if el.lower() in t1PlusManual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'we',\n",
       " 'President',\n",
       " 'Trump',\n",
       " 'You',\n",
       " 'your',\n",
       " 'look',\n",
       " 'media',\n",
       " 'TRUMP',\n",
       " 'We',\n",
       " 'WWIII',\n",
       " 'dying',\n",
       " 'asteroid',\n",
       " 'BANKNOTES',\n",
       " 'dangerous',\n",
       " 'economic',\n",
       " 'My',\n",
       " 'believed',\n",
       " 'knife',\n",
       " 'fight',\n",
       " 'NEWS',\n",
       " 'EMERGENCY',\n",
       " 'evil',\n",
       " 'invented',\n",
       " 'Iran',\n",
       " 'chaos',\n",
       " 'refuse',\n",
       " 'lying',\n",
       " 'me',\n",
       " 'Misleading',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'Market',\n",
       " 'market',\n",
       " 'stock',\n",
       " 'Wuhan',\n",
       " 'claims',\n",
       " 'China',\n",
       " 'conspiracy',\n",
       " 'killing',\n",
       " 'LMFAOOOOOO',\n",
       " 'lied',\n",
       " 'threat',\n",
       " 'crisis',\n",
       " 'Bank',\n",
       " 'panic',\n",
       " 'Stupidity',\n",
       " 'Hell',\n",
       " 'Death',\n",
       " 'died',\n",
       " 'Claims',\n",
       " 'Fears',\n",
       " 'Paranoia',\n",
       " 'weaponizing',\n",
       " 'hoax',\n",
       " 'claiming',\n",
       " 'Fear',\n",
       " 'campaign',\n",
       " 'Kills',\n",
       " 'my',\n",
       " 'death',\n",
       " 'I',\n",
       " 'fucked',\n",
       " 'our',\n",
       " 'rallies',\n",
       " 'base',\n",
       " 'screwed',\n",
       " 'emergency',\n",
       " 'weapon',\n",
       " 'Rallies',\n",
       " 'Clinton',\n",
       " 'DEAD',\n",
       " 'laugh',\n",
       " 'cure',\n",
       " 'DIE',\n",
       " 'lie',\n",
       " 'Hoax',\n",
       " 'lies',\n",
       " 'economy',\n",
       " 'dog',\n",
       " 'see',\n",
       " 'news',\n",
       " 'pretend',\n",
       " 'misinformation',\n",
       " 'suggest',\n",
       " 'cost',\n",
       " 'Communist',\n",
       " 'fuckin',\n",
       " 'kill',\n",
       " 'fear',\n",
       " 'financial',\n",
       " 'fucking',\n",
       " 'hysteria',\n",
       " 'kills',\n",
       " 'killed',\n",
       " 'Fake',\n",
       " 'war',\n",
       " 'impeachment',\n",
       " 'Killed',\n",
       " 'fears',\n",
       " 'Conspiracy',\n",
       " 'Cure',\n",
       " 'FAKE',\n",
       " 'fake',\n",
       " 'News',\n",
       " 'ECONOMY',\n",
       " 'MARKET',\n",
       " 'STOCK',\n",
       " 'money',\n",
       " 'LYING',\n",
       " 'LIES',\n",
       " 'Chernobyl',\n",
       " 'USSR',\n",
       " 'Japan',\n",
       " 'KILL',\n",
       " 'MY',\n",
       " 'DYING',\n",
       " 'feel',\n",
       " 'believe',\n",
       " 'Lies',\n",
       " 'MAGA',\n",
       " 'Propaganda',\n",
       " 'Media',\n",
       " 'antidote',\n",
       " 'PropagandaWars']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8166666666666667"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Table 1 words + stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary\n",
    "stems = [stemmer.stem(word) for word in t1]\n",
    "stems = list(set(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catastroph',\n",
       " 'appear',\n",
       " 'mine',\n",
       " 'look',\n",
       " 'wuhan',\n",
       " 'elect',\n",
       " 'our',\n",
       " 'china',\n",
       " 'conspiraci',\n",
       " 'bitch',\n",
       " 'refus',\n",
       " 'campaign',\n",
       " 'suppos',\n",
       " 'perceiv',\n",
       " 'die',\n",
       " 'feel',\n",
       " 'believ',\n",
       " 'i',\n",
       " 'hoax',\n",
       " 'evil',\n",
       " 'cat',\n",
       " 'accus',\n",
       " 'ralli',\n",
       " 'iran',\n",
       " 'magat',\n",
       " 'dog',\n",
       " 'your',\n",
       " 'ussr',\n",
       " 'market',\n",
       " 'my',\n",
       " 'fake',\n",
       " 'me',\n",
       " 'hell',\n",
       " 'secular',\n",
       " 'cure',\n",
       " 'wtf',\n",
       " 'blame',\n",
       " 'pretend',\n",
       " 'fukushima',\n",
       " 'nazi',\n",
       " 'genet',\n",
       " 'we',\n",
       " 'chines',\n",
       " 'chernobyl',\n",
       " 'impeach',\n",
       " 'you',\n",
       " 'breakthrough',\n",
       " 'trump',\n",
       " 'suggest',\n",
       " 'zombi',\n",
       " 'presidenti',\n",
       " 'chao',\n",
       " 'kill',\n",
       " 'hysteria',\n",
       " 'danger',\n",
       " 'hear',\n",
       " 'base',\n",
       " 'see',\n",
       " 'mislead',\n",
       " 'fight',\n",
       " 'war',\n",
       " 'money',\n",
       " 'screw',\n",
       " 'communist',\n",
       " 'hillari',\n",
       " 'martial',\n",
       " 'maga',\n",
       " 'cost',\n",
       " 'panic',\n",
       " 'paranoia',\n",
       " 'propaganda',\n",
       " 'junki',\n",
       " 'economi',\n",
       " 'laugh',\n",
       " 'claim',\n",
       " 'stock',\n",
       " 'dogbreath',\n",
       " 'bibl',\n",
       " 'japan',\n",
       " 'fundamentalist',\n",
       " 'foreign',\n",
       " 'weapon',\n",
       " 'stupid']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated by correctness of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_correct if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(correctPred), len(vocab_correct)))\n",
    "\n",
    "for i in range(len(correctPred)):\n",
    "    expl = correctPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7908496732026143"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7908496732026143"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_wrong if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(wrongPred), len(vocab_wrong)))\n",
    "\n",
    "for i in range(len(wrongPred)):\n",
    "    expl = wrongPred[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7037037037037037"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7037037037037037"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filtered vocab based on analysis words\n",
    "filtered_vocab = [el for el in vocab_list if stemmer.stem(el).lower() in stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stocks',\n",
       " 'markets',\n",
       " 'you',\n",
       " 'we',\n",
       " 'Trump',\n",
       " 'You',\n",
       " 'your',\n",
       " 'look',\n",
       " 'TRUMP',\n",
       " 'We',\n",
       " 'dying',\n",
       " 'dangerous',\n",
       " 'suggests',\n",
       " 'My',\n",
       " 'believed',\n",
       " 'refused',\n",
       " 'fight',\n",
       " 'evil',\n",
       " 'Iran',\n",
       " 'chaos',\n",
       " 'refuse',\n",
       " 'me',\n",
       " 'Misleading',\n",
       " 'junkies',\n",
       " 'Zombie',\n",
       " 'Fundamentalists',\n",
       " 'Market',\n",
       " 'market',\n",
       " 'stock',\n",
       " 'Wuhan',\n",
       " 'claims',\n",
       " 'China',\n",
       " 'conspiracy',\n",
       " 'killing',\n",
       " 'panic',\n",
       " 'Stupidity',\n",
       " 'Hell',\n",
       " 'died',\n",
       " 'Impeach',\n",
       " 'Claims',\n",
       " 'Paranoia',\n",
       " 'weaponizing',\n",
       " 'hoax',\n",
       " 'claiming',\n",
       " 'campaign',\n",
       " 'Kills',\n",
       " 'my',\n",
       " 'I',\n",
       " 'refuses',\n",
       " 'our',\n",
       " 'rallies',\n",
       " 'base',\n",
       " 'screwed',\n",
       " 'weapon',\n",
       " 'Rallies',\n",
       " 'laugh',\n",
       " 'cure',\n",
       " 'DIE',\n",
       " 'Hoax',\n",
       " 'economy',\n",
       " 'Chinese',\n",
       " 'dog',\n",
       " 'see',\n",
       " 'pretend',\n",
       " 'foreign',\n",
       " 'suggest',\n",
       " 'cost',\n",
       " 'looks',\n",
       " 'Communist',\n",
       " 'kill',\n",
       " 'Genetically',\n",
       " 'hysteria',\n",
       " 'seeing',\n",
       " 'kills',\n",
       " 'killed',\n",
       " 'Fake',\n",
       " 'Nazis',\n",
       " 'war',\n",
       " 'impeachment',\n",
       " 'Killed',\n",
       " 'conspiracies',\n",
       " 'Conspiracy',\n",
       " 'Cure',\n",
       " 'FAKE',\n",
       " 'fake',\n",
       " 'ECONOMY',\n",
       " 'MARKET',\n",
       " 'STOCK',\n",
       " 'elections',\n",
       " 'Appearing',\n",
       " 'money',\n",
       " 'Chernobyl',\n",
       " 'USSR',\n",
       " 'Fukushima',\n",
       " 'Japan',\n",
       " 'KILL',\n",
       " 'MY',\n",
       " 'feel',\n",
       " 'fighting',\n",
       " 'believe',\n",
       " 'MAGA',\n",
       " 'Propaganda',\n",
       " 'stupid']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary from filtered vocab\n",
    "myDict = dict()\n",
    "\n",
    "for i in range(len(filtered_vocab)):\n",
    "    myDict[filtered_vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of class associations of filtered vocab\n",
    "my_matrix = np.zeros((len(explanations), len(vocab_list)))\n",
    "\n",
    "for i in range(len(explanations)):\n",
    "    expl = explanations[i]\n",
    "    for j in range(len(expl)):\n",
    "        word = expl[j][0]\n",
    "        val = expl[j][1]\n",
    "        \n",
    "        if word in filtered_vocab:\n",
    "            if val > 0:\n",
    "                my_matrix[i, myDict[word]] = 1\n",
    "\n",
    "            if val < 0:\n",
    "                my_matrix[i, myDict[word]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no penalty\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        if el > 0:\n",
    "            rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalNonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.18333344931271334),\n",
       " ('unironic', 0.08000315840922398),\n",
       " ('NYT', 0.0628706663429014),\n",
       " ('Israelis', 0.0552038627822382),\n",
       " ('to', 0.04690367574080995),\n",
       " ('a', 0.04602305949756075),\n",
       " ('an', 0.045759952393576754),\n",
       " ('for', 0.04431488083685313),\n",
       " ('is', 0.04101546354827027),\n",
       " ('write', 0.03824383824026996),\n",
       " ('coronavirus', 0.038199697093392104),\n",
       " ('100', 0.03639169740551942),\n",
       " ('do', 0.03616979879774638),\n",
       " ('finding', 0.034518224116837234),\n",
       " ('likely', 0.030694457454544192),\n",
       " ('up', 0.03027192413811323),\n",
       " ('end', 0.026369508571869902),\n",
       " ('If', 0.026111692974492583),\n",
       " ('roughly', 0.024855474977400767),\n",
       " ('vaccine', 0.020854525884523082)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty for wrong class association of word\n",
    "tweet_scores = []\n",
    "totalNonZero = 0\n",
    "for row in my_matrix:\n",
    "    rowSum = 0\n",
    "    nonZero = 0\n",
    "    for el in row:\n",
    "        rowSum += el\n",
    "        if el != 0:\n",
    "            nonZero += 1\n",
    "            totalNonZero += 1\n",
    "    \n",
    "    if nonZero > 0:\n",
    "        score = rowSum/nonZero\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    tweet_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tweet_scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
